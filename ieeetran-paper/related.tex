\section{Related Work} \label{sec-related}

A variety of studies have been done on the classic problem of scheduling policies for task
assignment in distributed systems, and these policies are designed for different
circumstances and various requirements. TAGS \cite{balter} and SITA-V
\cite{Crovella:1998:TAD:277851.277942} deal with scheduling independent tasks among lots
of server nodes in a cluster, such as web servers that process HTTP requests. With the goal of
achieving low mean response time and low mean slow down, they reach consensus to
overcome the challenge of heavy-tailed task size distribution due to load unbalancing.
Task duplication (TD) \cite{Manoharan:2001:ETD:373047.373064} for massive parallel
processing has been analyzed for its effectiveness. Using task duplication based on
dependency graphs of tasks, TDS \cite{rohtua} and some other studies \cite{ahmad}
\cite{Dogan:2002:LDB:850943.853100} trade off between maximizing concurrency and
minimizing inter-processor communication with a primary objective of minimizing schedule
length and scheduling time. M. Silberstein et al. \cite{silberstein} propose a scheduling
policy for heterogeneous dynamic workloads composed of massively parallel short tasks.
Considering the communication cost and heterogeneous computing environment, some dynamic
task scheduling approaches \cite{Ahmad:1991:SLB:126283.126284} \cite{ucar} have been come up
with. An extended version of Condor MasterWork \cite{Heymann:2000:ASM:645440.652833}
dynamically measures the execution times of tasks and uses this information to dynamically
adjust the number of workers to achieve a desirable efficiency, minimizing the impact in
loss of speedup. Some of these previous works are old fashioned, as proposed
more than ten years ago. Our work has a different background and different
requirements with these prior ones. We aim at improving response time for a job
consisting of massive parallel computing-intensive tasks of a similar size.

Some job schedulers are designed for global administration over clusters
or datacenters. Most of them focus on overall throughput and fairness scheduling
\cite{isard2009}. HTCondor \cite{beowulfbook-condor} is a scheduler for coarse-grained
distributed parallelization of computationally intensive tasks in hybrid computing
environment, which is well known for its high throughput computing framework. ProActive is
a platform and middleware for parallel, distributed and multicore computing with a java
library provided and lots of features supported. ProActive Scheduler \cite{pascheduling} is a
job scheduler base on ProActive \cite{paprogramming} , which is similar with HTCondor.
Cooperating With ProActive Resource Manager \cite{parm} , it can be used with a variety of
computing resources. HTCondor and ProActive are widely deployed and used in grids and
clusters. Our $NO^2$ system is implemented to interact with ProActive Scheduler's
interfaces. Our work mainly addresses the outliers problem of massive parallel executions
and helps Scheduler to reduce the job completion time.

Speculative execution is borrowed from distributed file systems
\cite{Nightingale:2006:SED:1189256.1189258} and some other work
\cite{Su:2007:AIC:1294261.1294284} . Unlike those which treat parallel processes as a black box, our work launches
speculations in a more reasonable situation. With instrumentation, we change the view to a
white box.

It is attractive to use instrumentation for performance analysis, system modeling and debugging.
Although there are still many open research problems in developing
the instrument tools, it is among the most reasonable ways to understand complex system
behaviors. Magpie \cite{Barham:2004:UMR:1251254.1251272} is a tool chain for automatically
modeling workload of cluster and predicting performance with fine-grained instrumentation
in kernel, middleware, and application components. DTrace
\cite{Cantrill:2004:DIP:1247415.1247417} is an instrumentation framework on Solaris
Operating System, providing an option for lightweight dynamic instrumentation in kernel.
Similar approaches on other operating systems include the Linux Trace Toolkit (LTT)
\cite{Yaghmour:2000:MCS:1267724.1267726} and Event Tracing for Windows (ETW) \cite{etw}.
The PMaC Prediction Framework \cite{Carrington:2006:PPF:1134241.1708446} implements
an automated prediction model that takes into account attributes of
applications, input data, and target machine hardware (and other factors). The model
computes the expected performance as output. It is a generalized framework for similar 
requirements with our system. In our work, we use instrumentation for a simple clear goal. In a
specialized environment, our instrumentation is easier to implement than PMaC. As far
as we know, our work is the first case introducing an instrumentation approach to the
scheduling of Massive Parallel Processing.

Driven by the success of MapReduce \cite{dean} and its open-source implementation Hadoop,
lots of improvements \cite{Zaharia:2008:IMP:1855741.1855744}
\cite{Ananthanarayanan:2010:ROM:1924943.1924962} and adaptations
\cite{Srirama:2012:ASC:2304777.2304882} to varieties of applications have been done. Dryad
\cite{Isard:2007:DDD:1272998.1273005} is a general-purpose distributed execution engine
for coarse-grain data-parallel applications sharing the same goal with MapReduce but
extends the programming model of MapReduce. J. Ekanayake et al.
\cite{Ekanayake:2011:CTB:1990761.1990858} make a study on comparison and evaluation
of these technologies in scientific computing. There are various strategies to
choose which tasks to duplicate. Hadoop uses slots that free up to duplicate any task that
has read less data than the others after all tasks have been started. Dryad duplicates those
that have been running for longer than 75 percentage of task durations. LATE
\cite{Zaharia:2008:IMP:1855741.1855744} designs a new scheduling algorithm to robust
Hadoop with heterogeneity. Being cause-aware and resource-aware, Mantri
\cite{Ananthanarayanan:2010:ROM:1924943.1924962} detects and acts on outliers early in
their lifetime. All these prior scheduling strategies motivate our work with the
speculation idea. As MapReduce is designed for data-intensive computing, these works
naturally predict the progress of executions using data processing progress. But with
computing-intensive executions, this basic approche is not always valid.



