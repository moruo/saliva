\section{Related Work}

A variety of studies have been done on the classic problem of scheduling policies for task assignment to hosts in distributed systems, these works are distinguished to different circumstances and various requirements. TAGS \cite{balter} and SITA-V \cite{Crovella:1998:TAD:277851.277942} deals with scheduling independent tasks among lots of server nodes in cluster, such as web servers responding HTTP requests. With the goal of achieving low mean response time and low mean slow down, they reach consensus to overcoming the challenge of heavy-tailed in task size's distribution by load unbalancing. Task duplication (TD) \cite{Manoharan:2001:ETD:373047.373064} for massive parallel processing has been analyzed for its effectiveness. Using task duplication based on dependency graphs of tasks, TDS \cite{rohtua} and some other studies \cite{ahmad} \cite{1040891} trade off between maximizing concurrency and minimizing inter-processor communication with a primary objective of minimizing schedule length and scheduling time. M. Silberstein et al. \cite{silberstein} propose a scheduling policy for heterogeneous dynamic workload composed of massively parallel short tasks. Considering the communication cost and heterogeneous computing environment, some dynamic task \cite{99188} \cite{ucar} scheduling approach has been come up with. An extended version of Condor MasterWork \cite{Heymann:2000:ASM:645440.652833} dynamically measures the execution times of tasks and uses this information to dynamically adjust the number of workers to achieve a desirable efficiency, minimizing the impact in loss of speedup. Some of these previous works are old fashioned, which have been proposed more than ten years ago. Our work has a distinguished background and different requirements with these prior ones. We deal with improving response time for a job consisting of massive parallel computing-intensive tasks with nearly same size.

Some job schedulers are designed for global administration at a point of view of cluster or datacenter. Most of them considered overall throughput and fairness scheduling \cite{isard2009}. HTCondor \cite{beowulfbook-condor} is a scheduler for coarse-grained distributed parallelization of computationally intensive tasks in hybrid computing environment, which is well known for its high throughput computing framework. ProActive is a platform and middleware for parallel, distributed and multicore computing with a java library provided, lots of features supported. ProActive Scheduler \cite{pascheduling} is a job scheduler base on ProActive \cite{paprogramming} , which is similar with HTCondor. Cooperating With ProActive Resource Manager \cite{parm} , it can be used with a variety of computing resources. HTCondor and ProActive are widely deployed and used in grids and clusters. Our $NO^2$ system is implemented to interact with ProActive Scheduler's interfaces. Our work mainly considered the outliers problem of massive parallel executions and help Scheduler with reducing the job completion time.

Speculative execution is a shared idea from distributed file systems \cite{Nightingale:2006:SED:1189256.1189258} and some other work \cite{Su:2007:AIC:1294261.1294284} . Unlike this literature is focused on guessing along decision branches treating executions as a black box, our work launches speculations in a more reasonable situation. With instrumentation, we change the view to parallel executions to a nearly white box.

Using instrumentation for lots of performance analysis, system modeling and debugging requirements is attractive. Although there are many open research problems in developing the instrument tools, it is still the most reasonable way to understand complex system behavior. Magpie \cite{Barham:2004:UMR:1251254.1251272} is a toolchain for automatically modeling workload of cluster and predicting performance with fine-grained instrumentation in kernel, middleware, and application components. DTrace \cite{Cantrill:2004:DIP:1247415.1247417} is instrumentation framework on Solaris Operating System, provides an option for lightweight dynamic instrumentation in kernel. Similar approaches on different operating systems include the Linux Trace Toolkit (LTT) \cite{Yaghmour:2000:MCS:1267724.1267726} and Event Tracing for Windows (ETW) \cite{etw}. The PMaC Prediction Framework \cite{Carrington:2006:PPF:1134241.1708446} is an implementation of an automated prediction model that takes as parameters attributes of application software, input data, and target machine hardware (and possibly other factors) and computes, as output, expected performance. It is a generalized framework with our requirements. In our work, we use instrumentation for a simple clear goal. With a specialized environment, our instrumentation is easier to implementation than PMaC. As far as we know, our work is the first case introducing an instrumentation approach to the scheduling of Massive Parallel Processing.

Driven by the success of Map-Reduce \cite{dean} and its open-source implementation Hadoop, lots of improvements \cite{Zaharia:2008:IMP:1855741.1855744} \cite{Ananthanarayanan:2010:ROM:1924943.1924962} and adaptations \cite{Srirama:2012:ASC:2304777.2304882}  \cite{4811889} \cite{Ekanayake:2008:MDI:1488725.1488926} to varieties of applications have been done. Dryad \cite{Isard:2007:DDD:1272998.1273005} is a general-purpose distributed execution engine for coarse-grain data-parallel applications shared the same goal with Map-Reduce but extended the programming model of Map-Reduce. J. Ekanayake et al. \cite{5611496} \cite{Ekanayake:2009:DSA:1723206.1724844} also make a study on comparison and evaluation of using these technologies in scientific computing. There are various of strategies to choose which tasks to duplicate. After a threshold number of tasks havefinished, Map-Reduce duplicates all the tasks that remain. Hadoop uses slots that free up to duplicate any task that has read less data than the others after all tasks have started. Dryad duplicates those that have been running for longer than the 75 percentage of task durations. LATE \cite{Zaharia:2008:IMP:1855741.1855744} design a new scheduling algorithm to robust Hadoop with heterogeneity. With cause-aware and resource-aware, Mantri \cite{Ananthanarayanan:2010:ROM:1924943.1924962} detects and acts on outliers early in their lifetime. All these prior scheduling strategies motivate our work with the speculation idea. As Map-Reduce is design for data-intensive computing, these works naturally predict the progress of executions with data processing progress. But with computing-intensive executions, this implicit experience has been broken. Our work fills up this gap between computing-intensive and data-intensive task.


