\section{Introduction}

\IEEEPARstart{C}{loud} computing provides abundant computational resources to users. In a typical datacenter, over thousands of servers handle massive parallel tasks to support users' applications (e.g., information retrieval, analytical or scientific computation). Meanwhile, traditional clusters also meet much larger scale, commonly with thousands of CPU cores nowadays. On such a large-scale infrastructure, an efficient and robust processing framework is critical to users. This framework provides a viable way for users to organize and utilize the underlying tremendous computational resource pool. Specifically, it facilitates users' development/deployment of applications and provides guarantee on the quality of service (QoS). A set of easy-to-use but controlled interfaces are exported to users to code programs, and the framework takes charge of scheduling these programs to run on different server nodes in an efficient manner.

Tasks\footnote{In this paper, a job is the unit that a user submits to the framework, and tasks refer to what a job is split into when executed on a large multi-node infrastructure.} executed on these frameworks are generally classified into two types, the data-intensive and the compute-intensive. A data-intensive task issues a large amount of data I/O and its computation is relatively simple on these data, so data I/O usually dominates and spreads along the whole task. Meanwhile, a compute-intensive task performs only limited data I/O but spend most of time on computation.

Our observation is that most existing computing frameworks are suitable for data-intensive tasks but lack consideration for compute-intensive ones. The main reason behind involves the important scheduling component in these frameworks. Many previous investigations have shown that \textit{outliers} constitute a notorious performance killer in massive task processing. Outliers progress mush more slowly than peer tasks and dramatically delay the completion time of the whole job. Many common and unexpected factors may lead to outliers, like uneven task assignment, hardware/software problems, and network congestion. In order to address this issue, computing frameworks heavily rely on a smart scheduler, which should be capable of identifying outliers, timely aborting them, and re-executing related tasks on other healthy nodes.

Although there have emerged various large-scale computing frameworks, among which MapReduce is one of the most successful and representative, they lack a mechanism to estimate the progress of compute-intensive tasks and are consequently incapable of action to outliers. For example, MapReduce assigns a specific amount of data to a task and naturally uses the processed data amount or proportion as the indicator of the task's progress. This approach embodies an assumption that the task progress is approximately linear with data I/O. Furthermore, main subsequent improvements on the scheduler of MapReduce also hold this assumption. However, it is not the case in compute-intensive tasks. When this kind of tasks perform, data I/O activities mostly take a small portion of time and are skewed within a small interval. Therefore the traditional mechanism of progress estimation and outlier identification becomes problematic if directly applied to compute-intensive tasks.

Considering the significance and pervasiveness of compute-intensive tasks (e.g., analytical and scientific computing), we propose $N^2$, a framework designed for processing massive compute-intensive tasks in parallel. Our framework is based on ProActive and, more importantly, employs a scheduler that relies on instrumentation and sampling to precisely estimate task progress and accordingly cut off outliers. Similar with MapReduce, our framework achieves parallelism via SPMD (single program, multiple data), and supports tasks with minimal mutual communication, rather than the message-passing ones (e.g., MPI programs).

There are several challenges to make our solution applicable to the production environment. (1) Instrumented programs may encounter unacceptable overheads. For example, we instrument function calls to reflect the position of program that the task is executing, in the case when the program is well structured by functions. But if a too frequently called function is chosen, the overheads shall increase. To meet such a challenge, we introduce a sampling phase to deliberately yet automatically select what functions/instructions to instrument. (2) Even though the instrument points are wisely selected, there is no guarantee that these points are touched linearly along with the task progress. In the task duration, some instrument points appear temporally dense in the trace while others are sparse. So we have to design a scheduling policy to pick up potential outliers based on such skewed data. We borrow a data mining technique, the k-mean clustering, to distinguish different task states from their traces. (3) There exist many legacy code in scientific computing. We have to provide concise interfaces to execute them and integrate all the instrumentation, sampling and scheduling phases to form a user-friendly workflow. This target is also achieved in our design of framework.

To sum up, we make the following contributions in $N^2$.
\begin{itemize}
\item We design and implement an easy-to-use, efficient and robust framework for parallel execution of massive compute-intensive tasks.
\item We create an instrumentation-based technique to estimate the progress of compute-intensive tasks. Our approach introduces a sampling phase before final instrumentation to collect program characteristics, which enables wise selection of instrumentation points and better estimation of progress.
\item At runtime, we design a scheduling policy based on the k-mean clustering method to identify outliers. This method is robust to uneven instrument points and diversity between data sets.
\end{itemize}

%\section{System Overview}
