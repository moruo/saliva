\section{Implementation}

For a clear understanding, we illustrate some important details of $NO^2$'s implementation
in this section. The technologies we used in development and some implementation tricks
hidden behind are introduced in brief.

We developed $NO^2$'s instrument tool with a static instrumentation fashion based on
DynInst \cite{Dyninst-Deconstruction} library. DynInst library is an Application Program
Interface (API) \cite{dyninstapi} that permits the insertion of code into a static binary
code or a running process. The goal of this API is to provide a machine independent
interface to permit the creation of instrumentation tools and applications. And $NO^2$'s
implementation of function hits statistics is based on an example of DynInst tools, called
CodeCoverage.

Generally, there are three code snippets for the instrument points selector: the initial
one for creating some data structures for statistics, the final one for writing back the
function hits data to a file, and the common used one for record a function call. For the
instrumenter, the initial one for opening a file, the final one for closing a file, and
the commonly used one just for increasing one to the number of tracepoints then writing it
to the file. All the code snippets are carefully assembled with DynIsnt API and
manipulated with the original binary.

For the purpose of excellent performance, the instrumenter completes the assembly of file
operations using the low level system call \emph{open}, \emph{write}, and \emph{close} in
Linux Opreating System. The file for saving traces is created in the directory which is
mounted as a RAM file system. The RAM file system uses memory as a disk in the Virtual
File System (VFS) level supported by the Linux Operation System. It means that only
several memory operations overhead is introduced into the instrumentation of each function
hit.

We provide a tasks speculation implementation with ProActive Scheduler
\cite{pascheduling}. The ProActive Scheduler can be interacted with a control script. The
control script is interpreted in javascript language based on a script engine, called
'Rhino', built-in integrated with the distribution of Java Standard Edition 6(Java SE6).
ProActive Scheduler provides the ability of interactive with the scheduler instance at
runtime by the fact that script engine can access and invoke the objects and methods in
Java.

Our task scheduling policy generator calls the outliers clustering module to establish
outliers clustering with updated trace data. Then maintaining a blacklist of nodes, each
node in the blacklist has a penalty value. The penalty could be eliminated if no further
outliers found on a node, and it could be removed from the blacklist. In summary, the
blacklist is used for keeping the speculative tasks away from the most possible nodes that
may produce outlier tasks. For each outlier, the policy generater creates an urgency job
description that has a higher priority in the ProActive Scheduler. This urgency job has
only one speculative task. Then all these urgency jobs are submitted to the ProActive
Scheduler in a speculation priority order. The constraint that no speculation takes place
on irregular nodes, can be achieved with a selection script and a claim of nodes selection
in the job description. The default speculation interval is three seconds, means that the
interactive with the job scheduler is moderate. And each update of traces information has
little communication traffic, only several bytes indicating the number of tracepoints are
transmitted. This lightweight implementation is acceptable to the job scheduler and the
impact of performance can be ignored roughly.

We provide three extra shell scripts for splitting, wrapping native program and merging
the results if needed. The shell scripts will be submitted and executed as normal tasks on
job scheduler. They can be used immediately in ProActive Scheduler. But it doesn't mean
any dependence, It is easy to export to other schedulers with tiny modification. Taking a
picture rendering job for example, there may be three phases for this job: 1. splitting
the image into small pieces, 2. rendering them in parallel, 3. merging the rendered parts.
These three phases exactly match $NO^2$'s three shell script template : split script,
operation script and merge script. Users just need to add an image cutting command into
the split script, which may be optional for other jobs. The default procedure, copying
input files to the destinations, can also be modified if needed. In the operation script,
the native program must be expressed as a command, a daemon process for traces
transferring is also launched as default. When all parts of the results has been
collected, a merge image command is executed in the merge script, optional for other jobs
too. As described above,  users have an extreme flexibility to make their native programs
massive parallel. On the other hand it means $NO^2$ do not provide any file transfer or
data placement service and so on, which have been provided by job schedulers or a
distributed file system of cluster itself.
